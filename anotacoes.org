#+TITLE: Pesquisa - Análise da produção científica da UFRJ: um comparativo com diferentes fontes informacionais
#+AUTHOR: Gabriel Alves Vieira
#+BIBLIOGRAPHY: Bibliometry apalike

* Idéia Geral
- Obter os dados da produção da UFRJ em diferentes bancos de dados acadêmicos e compará-los em termos de:
  + Cobertura
  + Métricas
- A comparação será feita tanto a nível institucional (toda a produção), quanto a nível de assunto/disciplina.
- As seguintes plataformas serão utilizadas (até que não sejam mais):
  + Scopus
  + Web of Science
  + Dimensions
  + +Microsof Academic+


* Análises/perguntas adicionais
** Será que a UFRJ está preparada/se adequando com relação ao Plan S?
- Saber quando o Plan S foi anunciado seria interessante para fazer o recorte temporal. **R: Segundo a wikipedia, 2018**
- Isso requereria fazer uma análise da proporção de artigos abertos nos diferentes padrões (ouro, verde, etc...) ao longo dos anos


* Semana do dia 01/06/2022
** Recuperando dados das plataformas para a UFRJ
*** Scopus
- Dentre seus filtros, há o **AF-ID (Affiliation ID)**, um identificador único atribuído às organizações afiliadas aos autores.
  + É possível usar o [[https://www.scopus.com/search/form.uri?display=affiliationLookup#affiliation][Scopus affiliation search]] para obter o AF-ID de uma instituição.
  + No caso da UFRJ, o AF-ID é **60000036**
- Também há o campo do **DOI** e um **Scopus Author Identifier**, que provê identificadores únicos para publicações e autores.
- Uma limitação do Scopus é que ele retorna apenas os primeiros 2000 resultados. Se pelo menos ele picotasse os resultados em arquivos distintos...
**** Estratégia de busca
- **Limite de documentos por download: 2000**
- Total de **27029 resultados** para a [[https://www.scopus.com/results/results.uri?sort=plf-f&src=s&sid=9b25ffb0e296e3dfcb89779e93f9f20e&sot=a&sdt=a&sl=94&s=AF-ID%28%22Universidade+Federal+do+Rio+de+Janeiro%22+60000036%29+AND+PUBYEAR+%3e+2016+AND+PUBYEAR+%3c+2022&origin=searchadvanced&editSaveSearch=&txGid=48bd8fdc332758f37492bfea8ed8b46d][UFRJ no período entre 2016-2021]].
- Aparentemente, uma boa **combinação de filtros** para obter arquivos com menos do que 2000 records talvez seja **instituição + ano + área**.
  + Entretanto, se pegarmos [[https://www.scopus.com/results/results.uri?sort=plf-f&src=s&sid=9b25ffb0e296e3dfcb89779e93f9f20e&sot=a&sdt=a&sl=94&s=AF-ID%28%22Universidade+Federal+do+Rio+de+Janeiro%22+60000036%29+AND+PUBYEAR+%3e+2016+AND+PUBYEAR+%3c+2022&origin=searchadvanced&editSaveSearch=&txGid=48bd8fdc332758f37492bfea8ed8b46d][a produção da UFRJ entre 2016-2021]], podemos ver que dá pra baixar toda a produção da maioria dos subjects de uma vez pelo site (menos de 2000 records de Material Science pra baixo)
  + Os outros 8 campos precisariam de ser divididos por ano para serem baixados (a menos que eu por acaso use a API, para a qual aparentemente o limite é de 5000 por request)
**** Scopus API
- Já tenho uma key. Mas a chave para não assinantes vem com mta missing data.
- Um bom wrapper pra API da Scopus é [[https://pybliometrics.readthedocs.io/en/stable/][pybliometrics]].

*** Web of Science
- Aparentemente, **sem acesso algum à API** sem ter que pagar.
- Acesso pelo **Portal Periódicos CAPES**
- A UFRJ tem uma lista enorme de possíveis nomes para afiliações. Acho que não vai precisar desambiguar nada.
**** Estratégia de busca
- **Limite de documentos por download: 1000**
  + Obs: Se não precisarmos baixar as referências citadas. Se precisarmos, aí cai para 500 documentos por download.
- Total de **26302 resultados** para pesquisa: [[https://www.webofscience.com/wos/woscc/summary/4aec5116-c239-4ec0-94ca-59b5046636fa-3d204a97/relevance/1][Universidade Federal do Rio de Janeiro, de 2016-01-01 a 2021-12-31]]
- Divisão por **Categorias Web of Science**: Todas possuem menos de 1000 items.
  + São 246 categorias, segundo a opção "analyze results" do WoS
  + Entretanto, uma publicação pode pertencer a mais de uma categoria
  + Exportei a tabela gerada pelo analyze results e somando as porcentagens de cada categoria, deu mais de 150%.
  + Logo, vou ter que **eliminar redundância depois (provavelmente nos datasets da Scopus tbm)**

*** Dimensions
- Dá pra ganhar uma chave API para [[https://www.dimensions.ai/scientometric-research][pesquisa cientométrica]].
  - Obs: Eu **não posso disponibilizar os dados baixados da Dimensions**. Lembrar de só manter o código necessário para baixar ele.
  - A Dimensions tem uma página ([[https://api-lab.dimensions.ai/][Dimensions API Labs]]) com vários **Jupyter Notebooks** que mostram como usar a API. Quando tiver a key, **dar uma olhada lá para ver se algo pode ser adaptado pro meu uso**.
- Não parece ter um identificador único para instituição.
- Entretanto, tem uma API gratuita para pegar o **número de citações por publicação**. Com isso, já **daria para calcular h-index e outras métricas**.
  - É bem simples e pode usar tanto DOI quanto PMID (PubMed ID) ou Dimensions ID.
  - [[https://metrics-api.dimensions.ai/doi/10.7717/peerj.6271][Exemplo com o DOI]]
- A altmetria por artigo aparentemente pode ser obtida usando a [[https://docs.dimensions.ai/dsl/examples.html#altmetric][API paga]].
- Imagino que tenha como recuperar a **lista de citações** com a API tbm.
- Usa o [[https://www.grid.ac/][GRID]] como **identificador único institucional**
  - GRID para "Federal University of Rio de Janeiro": **grid.8536.8**
**** Problema: Pesquisa avançada não disponível
- [[https://dimensions.freshdesk.com/support/solutions/articles/23000018802-how-to-search-in-dimensions#Advanced-search**][Advanced Search only available in Dimensions Analytics]]
  + Isso é problemático, já que a forma mais adequada de se obter as pesquisas com afiliação à UFRJ é pelo GRID (Research organization ID da pesquisa avançada)
  + Para ver mais sobre os campos de metadados da pesquisa avançada: [[https://dimensions.freshdesk.com/support/solutions/articles/23000023683-using-the-advanced-search-in-dimensions][Using the advanced search in Dimensions]]
  + Para além disso, nem todos os filtros estão disponíveis na versão grátis da Dimensions. E os que estão são bem ruins de usar para restringir os matches e recuperar a informação em pequenas partes.
    - Por exemplo, ao clicar no filtro "Source Title", para baixar por revista, o dimensions não faz que nem Scopus/WoS e te mostra a lista das revistas, onde vc pode selecionar uma a uma para recuperar tudo. Em vez disso, mostra as revistas mais comuns e se vc clica em 'more', ele só abre uma caixa para vc digitar a revista do seu interesse.
    - *CORREÇÃO:* Mas indo em *Analytical Views, dá para ver todas as revistas/categorias de pesquisa. Não dá pra baixar a lista, mas dá pra ver, ao menos.
  + +Supostamente, eles têm mecanismos para desambiguar o nome de organizações ([[https://dimensions.freshdesk.com/support/solutions/articles/23000023683][org_names_raw and proximity searches]]), então talvez dê para botar "UFRJ" e limpar o dado depois.+
  + Aparentemente, o match acontece só com o texto disponível, não (ou ao menos não só) com a afiliação. Isso pode gerar muitos falsos matches (pesquisas não feitas pela URFJ, mas que a citam em algum momento)
**** Estratégia de busca
- Limite de documentos/download: *500*
- Total de [[https://app.dimensions.ai/discover/publication?search_mode=content&search_text=%22UFRJ%22+OR+%22Federal+University+of+Rio+de+Janeiro%22+OR+%22Universidade+Federal+do+Rio+de+Janeiro%22&search_type=kws&search_field=full_search&or_facet_year=2016&or_facet_year=2017&or_facet_year=2018&or_facet_year=2019&or_facet_year=2020&or_facet_year=2021][resultados]]: *85429*
- Dadas as limitações da ferramenta de busca gratuita da dimensions, só consigo pensar em duas opções para usá-la:
  1. Pedir que eles enviem os dados da query "(org_id:grid.26999.3d) AND (date:[2016-01-01 TO 2020-12-31])"
  2. Pedir uma chave gratuita para a dimensions analytics API [[https://digital-science.ccgranttracker.com/Login.aspx?ReturnUrl=%2f][aqui]].

*** Microsoft Academic
- Aparentemente, ela não está mais funcionando...
- "Editor’s note, May 4, 2021 – In a recent blog post, it was announced the **Microsoft Academic website and underlying APIs will be retired on Dec. 31, 2021**."

* Semana do dia 06/06/2022
- Acabo de checar que os três maiores bancos de dados (Scopus, WoS e Dimensions) possuem campos com o número de citações em seus exports. Logo, dá pra calcular métricas como o h-index com todos e fazer uma comparação.
- O mesmo é válido para averiguar adesão ao Open Access, já que todas elas possuem um campo que diz não só se é acesso aberto, mas de qual tipo (gold, green...)

* Semana do dia 13/06/2022
** Lens.org
- Minha chave API vai até o dia 17/06
- Fiz o download pelo site mesmo. O máximo de records é 50K, que é mais do que eu preciso de toda forma.
- O csv gerado parece ter todos os dados que a gente precisa: doi (apesar de faltar em algumas entradas), número de citações, se é open acess (True or False) e tipo/cor do open access (green, gold, etc.)
** Dúvida para reunião com Jacque:
- Será que eu vou precisar das citações dos artigos da WoS? Se sim, vou ter que fazer o download de 500 artigos por vez.
** R
*** readr vs base R
- *read.delim* (separado por ponto) e funções derivadas são do *R base*. Enquanto isso, *read_delim* (separado por underscore) e derivados são do *readr*, pacote pertencente ao tidyverse. As funções do readr possuem mais parâmetros/funcionalidades, como selecionar colunas específicas por nome para serem lidas.
- Por outro lado, tem a função *fread()* (file read) do pacote *data.table*, que também possui várias funcionalidades extras
*** do.call
- do.call() is useful when using objects with multiple lists.
- For instance, when using do.call(rbind, lists), you will merge all lists into asingle dataframe.
- [[https://stackoverflow.com/questions/10801750/whats-the-difference-between-lapply-and-do-call][Link discutindo a diferença]]
- lapply: applies a given function for each element in a list,so there will be several function calls.
- rbind: binds all arguments row-wise.
- do.call: applies a given function to the list as a whole,so there is only one function call.
*** rbindlist e bind_rows
- Ambas fazem o mesmo que do.call(rbind, list_of_dataframes): pegam uma lista de dataframes e juntam em uma df única
- rbindlist() é do pacote *data.table*.
- bind_rows() é do *dplyr* (parte do tidyverse).

* Semana do dia 20/06/2022
** Diagrama de Venn e Upset Plot
- Ambas formas de visualizar interseções entre conjuntos
- Vários pacotes do R fazem esses plots
- Para Venn: Usei o ggVennDiagram (tem outras)
- Para Upset Plot: Duas bibliotecas principais:
  - [[https://jokergoo.github.io/ComplexHeatmap-reference/book/upset-plot.html][ComplexHeatmap]]
  - [[https://cran.r-project.org/web/packages/UpSetR/vignettes/basic.usage.html][UpsetR]]
** NA's na coluna dos DOI's
- Múltiplas vezes, vi que a coluna dos DOIs está com NAs. Isso em múltiplos tipos de publicações, incluindo artigos.
- Jacque comentou no projeto que iremos usar não apenas DOIs, mas também nome, ano e título do periódico para determinar se o item ocorre em mais de uma base. Essa é uma forma de lidar com isso.
- Entretanto, ainda não faço idéia de como implementar isso, de forma a conseguir os sets individuais. Vou ter que quebrar um pouco a cabeça nessa questão...

** Correção do projeto
- Jacque enviou o projeto corrigido. Resolvi retirar a parte de acesso aberto.
- Comecei a ler os papers que ela sugeriu.

** Semana que vem
- Uma coisa válida a ser testada:
  - Pode ser que *o artigo possua DOI, mas esse esteja faltando na database*
  - Ao mesmo tempo, tem o [[https://www.crossref.org/guestquery/][CrossRef DOI lookup]], onde dá pra, com base nas informações como título e afins, procurar o DOI.
  - Tentar recuperar o DOI de alguns *artigos* usando esse serviço
  - Se funcionar, vale a pena tentar fazer isso programaticamente (o DOI lookup suporta recuperação por xml)

* Semana do dia 04/07/2022
** 07/07/2022 - Baixando dados da Scopus/WoS
*** Wos
- [[https://www.webofscience.com/wos/woscc/summary/d64143ad-1d96-4437-a354-953ec49d62ed-420e1ffa/relevance/1][Query link]]
- Baixando "Full Records with Citations" (500 records/download)

* Semana do dia 18/07/2022
- Submeti um application pra Dimensions pra conseguir acesso gratuito à API.
- Hora de montar os gráficos com todos os dados pra apresentação pra CPG. Para isso, preciso baixar os dados da Scopus e alguns da Dimensions.
- Ticket atendimento TIC: 0410029

* Semana 01/08
** Instalação Web Servers
- Registro de domínio: labinfomet.studio (feito com a conta de estudante do github e válido por um ano)
- Rstudio: bem simples de configurar (inclusive com o nginx)
- Jupyterhub: também consideravelmente fácil. Não esquecer de, caso for hostear o serviço fora da raiz (ex: www.domain.lts/jupyter), setar a variável c.JupyterHub.base_url para o caminho adequado (ex: '/jupyter').

* Reunião reitora (19/08)
- Aparentemente, a UFRJ vem perfomando mal em termos de "citações" em dois rankings anuais: QS (Quacquarelli Symonds) e THE (Times Higher Education).
- Surgiram em 2004, como um único ranking. Depois, se "diferenciaram" em QS e THE.
- Ambos baseados em dados da Scopus

* Semana 22/08/2022
** Artigo Guerrero-Bote et al
- "... Dimensions is a clear alternative for carrying out citation studies, being capable of rivalling Scopus. But the reliability and validity of its field classification scheme were questioned. This scheme is not based on journal classification systems as it is in WoS or Scopus, but on machine learning."
- "This is the case with the study of Huang et al. (2020) which compared WoS, Scopus, and Microsoft Academic and their implications for the robustness of university rankings."
- *Dimensions and GRID*:
  - "Bibliographic databases often give bibliometric studies problems with author affiliations which usually do not include standardized names of institutions. One of the improvements that Dimensions incorporates is the mapping of author affiliations in documents to an entity list for organizations involved in research. This is the GRID (Global Research Identifier Database) system (Hook et al., 2018). This mapping is not an addition to but a replacement for author affiliations. If this mapping is rigorous and complete, it is an important improvement. But if the list of organizations or the mapping is incomplete, this could be a major problem because there would be loose documents without any possibility of associating them with institutions or countries, thus leaving the output of the institutions and countries affected incomplete."
- *Matching of publications*
  - "For validation, all the reference’s data were compared: DOI, year of publication, authors, title, publication, volume, issue, and pages. The last three were compared both numerically and alpha- numerically. The comparison of each field generated a numerical score corresponding to the number of matching characters with some adjustments, for which the Levenshtein1 distance was used as in Guerrero-Bote et al. (2019) and Visser et al. (2020)."
  - Talvez devesse *Olhar os artigos citados para ter uma idéia de como o score foi calculado*
- *Nossa abordagem (talvez)*
  - Como vamos lidar com um subset bem menor de dados, podemos usar uma metodologia diferente.
  - Identificar as mesmas publicações foi um processo feito em múltiplas fases. As fases são descritas no paper. Talvez, uma idéia seja:
    - Juntar todas as planilhas em uma só (bibliometrix)
    - "Quebrar" por base de dados
    - Dar match em todos os arquivos que tiverem o mesmo DOI
    - Daí, alguns parametros podem ser avaliados:
      - Título com alto nível de similaridade (levenshtein)
        - Levenshtein: o número mínimo de edições (inserções, deleções ou substituições para converter uma palavra em outra)
      - Mesmo ano de publicação
      - Mesmo periódico (Devo me preocupar com variação de nome do periódico?)
      - Mesmos autores (Devo me preocupar com ordem do nome e afins, ou só usar o primeiro autor?)
* Festival do Conhecimento UFRJ - Painel Temático: Indicadores de desempenho e avaliação institucional
** Introdução Full
- Boa tarde a todos, sejam bem-vindos ao Painel Temático Indicadores de desempenho e avaliação institucional.
- Meu nome é Gabriel e eu serei o mediador dessa atividade. Sou aluno de doutorado aqui da UFRJ (mais especificamente do IBqM) e trabalho no Lab. de Informação e Metrias em Ciência e Tecnologia, liderado por uma das palestrantes desse painél, a Prof. Jacqueline Leta. Gostaria de agradecer primeiramente a ela pela oportunidade de mediar uma mesa com um tema tão interessante e também agradecer aos palestrantes que se disponibilizaram para vir aqui e compartilhar um pouco do seu conhecimento sobre indicadores e avaliação institucional. E por último, gostaria de agradecer a todos os que estão prestigiando não só esse painel quanto o Festival do Conhecimento de forma geral.
- A primeira apresentação será da minha queridíssima orientadora, Dra. Jacqueline Leta,
  - Que possui graduação em Ciencias Biológicas pela UFRJ (1992), tendo obtido os títulos de mestrado e doutorado em Gestão, Educação e Difusão em Ciências pela UFRJ nos anos de 1995 e 1999, respectivamente. Desde 1994, tem liderado pesquisas no campo da Bibliometria e Cienciometria, com ênfase nas análises da comunicação científica brasileira, principalmente nas temáticas Ciência e Saúde e Ciência e Gênero. Foi pesquisadora da Pró-Reitoria de Pesquisa da USP (2000 a 2002), membro da Comissão Permanente de Indicadores de C&T, do MCT (2003) e membro da Comissão de Avaliação do Diretório dos Grupos de Pesquisa no Brasil (2010). Participa do corpo de editores de revistas nacionais e internacionais, sendo, desde 2015, integrante da comissão editorial dos Anais de Academia Brasileira de Ciências. Membro do Comitê Executivo da International Society of Scientometrics and Informetrics (ISSI) de 2013 a 2017, organizou o 1o Encontro Brasileiro em Bibliometria e Cientometria em 2008 e a Conferência Internacional em Cientometria e Informetria em 2009, ambos no Rio de Janeiro. Realizou o pós-doutorado na Univ Católica de Leuven em 2005, sob a coordenação do Dr. Glanzel, um dos principais nomes da cientometria mundial. De 2015, foi eleita coordenadora do Programa de Pós-Graduação em Ciência da Informação, convênio IBICT - UFRJ, do qual participa como docente permanente desde 2009. Atualmente é chefe do Programa de Educação, Gestão e Difusão em Ciências, do Instituto de Bioquímica Médica, onde tem vínculo de docente associada. Ao longo da carreira, orientou dezenas de alunos da graduação e da pós-graduação e publicou um livro, capítulos de livros e diversos artigos sobre a ciência brasileira. A professora Jacqueline discorrerá sobre indicadores de desempenho e os principais processos de avaliação do Brasil.
  - A segunda apresentação será conduzida pela Dra. Daniela Uziel,
    - Que possui graduação em Medicina pela Universidade Federal do Rio de Janeiro (1995), mestrado em Ciências Biológicas (Biofísica) pela Universidade Federal do Rio de Janeiro (1997), doutorado em Ciências (Biofísica) pela Universidade Federal do Rio de Janeiro (2001), após período "sanduíche" na França (Inserm U371) e Alemanha (FSU Jena) e um mais recente doutorado (2019) Políticas Públicas, Estrategias e Desenvolvimento (IE-UFRJ). É Professora Associada da Faculdade de Farmácia da Universidade Federal do Rio de Janeiro. Atua desde março de 2019 como Coordenadora de Inovação do Centro de Ciências da Saúde, desde setembro de 2019 no Comitê de Inovação e desde novembro de 2020 como coordenadora do Programa de Gestão de Indicadores de Desempenho (GID) da Pro-reitoria de Pós-graduação e Pesquisa da UFRJ. Foi Pesquisadora Visitante do IPEA (Centro de Ciência, Tecnologia e Sociedade; sede Rio de Janeiro) entre outubro de 2018 e março de 2020. Atua em parceria com docentes de outras áreas do conhecimento da UFRJ e da UFF nas Disciplinas Integradas de Empreendedorismo da UFRJ, que recebeu o prêmio de educação empreendedora (rodada estadual) do Sebrae em 2019. Desde 2012 trabalha com empreendedorismo acadêmico de alta tecnologia, inovações na área biomédica e fomento à formação de um cluster de ciências da vida no estado do Rio de Janeiro. A professora Daniela irá compartilhar conosco sua experiência com o escritório de gestão de indicadores que ela coordena aqui na UFRJ.
  - A terceira apresentação será do Dr. Rogério Mugnaini,
    - Que é estatístico (2001), mestre (2003) e doutor (2006) em Ciência da Informação. Compõe o corpo docente do Departamento de Informação e Cultura da Escola de Comunicações e Artes (ECA) da Universidade de São Paulo (USP), onde coordena o CiMetrias (Grupo de Pesquisa em Métricas da Ciência e Tecnologia). Atua como professor e atual coordenador do Programa de Pós-Graduação em Ciência da Informação da ECA-USP. Tem experiência na coordenação de projetos de pesquisa nacionais e internacionais, nos seguintes temas: bibliometria, cientometria, avaliação de produção científica nacional, indicadores, fontes de informação, política científica e ciência aberta. Realizou estágios de pesquisa nas universidades: Universidad Carlos III de Madrid e Leiden University. Integrou comissões, tanto no âmbito da universidade (Vice-Presidente da Comissão de Credenciamento do Programa de Apoio às Publicações Científicas Periódicas da USP, entre 2010 e 2016) quanto em âmbito nacional (Avaliações Trienal 2013 e Quadrienais 2017 e 2021 da área de Comunicação e Informação - CAPES). Membro filiado da Associação Nacional de Pesquisa e Pós-Graduação em Ciência da Informação (ANCIB) e International Society for Scientometrics and Informetrics (ISSI). Exerceu atividade profissional na BIREME/OPAS/OMS (2003-2009), tendo desenvolvido o Módulo de Bibliometria do Projeto SciELO. O professor Rogério irá abordar em sua fala os indicadores na avaliação Capes.
  - A quarta e última apresentação desse painel será realizada pela Dra. Samile Vanz,
    - Que é professora associada do Departamento de Ciências da Informação, do Programa de Pós-graduação em Comunicação (PPGCOM UFRGS) e do Programa de Pós-graduação em Ciência da Informação da Universidade Federal do Rio Grande do Sul (PPGCIN UFRGS). Graduada em Biblioteconomia pela Universidade Federal do Rio Grande do Sul (1999), mestre e doutora em Comunicação e Informação pelo PPGCOM UFRGS (2004 e 2009), com estágio sanduíche na Dalian University of Technology (China, 2007-2008). Pós-doutorado pela Universidad Carlos III de Madrid (Espanha, 2016). Editora da revista Em Questão (2014 - ). Desenvolve pesquisas na área de Comunicação Científica, com ênfase na produção de indicadores científicos, bibliometria, colaboração científica, análise de citação, análise de co-citação e rankings universitários.Tem experiência acadêmica e profissional na área de Planejamento, gestão e arquitetura de bibliotecas. A Profa. Samile discorrerá sobre os rankings internacionais e as universidades brasileiras.
  - Então, dado o calibre dos apresentadores e os temas abordados por ele, certamente essa atividade será muitíssimo interessante e esclarecedora. Lembrando que teremos um espaço para dúvidas e discussões assim que todas as apresentações forem concluídas. Mais uma vez muito obrigado a todos os envolvidos e espero que aproveitem a atividade.


** Introdução Resumida
- Boa tarde a todos, sejam bem-vindos ao Painel Temático Indicadores de desempenho e avaliação institucional.
- Meu nome é Gabriel e eu serei o mediador dessa atividade. Sou aluno de doutorado aqui da UFRJ (mais especificamente do IBqM) e trabalho no Lab. de Informação e Metrias em Ciência e Tecnologia, liderado por uma das palestrantes desse painél, a Prof. Jacqueline Leta. Gostaria de agradecer primeiramente a ela pela oportunidade de mediar uma mesa com um tema tão interessante e também agradecer aos palestrantes que se disponibilizaram para vir aqui e compartilhar um pouco do seu conhecimento sobre indicadores e avaliação institucional. E por último, gostaria de agradecer a todos os que estão prestigiando não só esse painel quanto o Festival do Conhecimento de forma geral.
- Esse painel foca em um dos mecanismos de avaliação e monitoramento da ciência: a avaliação por indicadores de desempenho. Concebida nos anos 1960, essa nova forma vai na contramão da avaliação por pares e apresenta-se como um mecanismo externo pautado em critérios objetivos em torno de indicadores de resultados ou desempenho (outputs), somando-se, assim, aos critérios já existentes de insumo (inputs). Desde então, estes indicadores vêm sendo utilizados em avaliações de desempenho institucional, como na avaliação da Capes e nos rankings universitários.
- A primeira apresentação desse tema tão relevante e interessante será da minha queridíssima orientadora, Dra. Jacqueline Leta,
  - Que possui graduação em Ciencias Biológicas pela UFRJ (1992), tendo obtido os títulos de mestrado e doutorado em Gestão, Educação e Difusão em Ciências pela UFRJ nos anos de 1995 e 1999, respectivamente. Desde 1994, tem liderado pesquisas no campo da Bibliometria e Cienciometria, com ênfase nas análises da comunicação científica brasileira, principalmente nas temáticas Ciência e Saúde e Ciência e Gênero. Realizou o pós-doutorado na Univ Católica de Leuven em 2005, sob a coordenação do Dr. Glanzel, um dos principais nomes da cientometria mundial. De 2015, foi eleita coordenadora do Programa de Pós-Graduação em Ciência da Informação, convênio IBICT - UFRJ, do qual participa como docente permanente desde 2009. Atualmente é chefe do Programa de Educação, Gestão e Difusão em Ciências, do Instituto de Bioquímica Médica, onde tem vínculo de docente associada. Ao longo da carreira, orientou dezenas de alunos da graduação e da pós-graduação e publicou um livro, capítulos de livros e diversos artigos sobre a ciência brasileira. A professora Jacqueline discorrerá sobre indicadores de desempenho e os principais processos de avaliação do Brasil.
  - A segunda apresentação será conduzida pela Dra. Daniela Uziel,
    - Que possui graduação em Medicina pela Universidade Federal do Rio de Janeiro (1995), mestrado em Ciências Biológicas (Biofísica) pela Universidade Federal do Rio de Janeiro (1997), doutorado em Ciências (Biofísica) pela Universidade Federal do Rio de Janeiro (2001), após período "sanduíche" na França (Inserm U371) e Alemanha (FSU Jena) e um mais recente doutorado (2019) Políticas Públicas, Estrategias e Desenvolvimento (IE-UFRJ). É Professora Associada da Faculdade de Farmácia da Universidade Federal do Rio de Janeiro.Desde 2012 trabalha com empreendedorismo acadêmico de alta tecnologia, inovações na área biomédica e fomento à formação de um cluster de ciências da vida no estado do Rio de Janeiro. Atua desde março de 2019 como Coordenadora de Inovação do Centro de Ciências da Saúde, desde setembro de 2019 no Comitê de Inovação e desde novembro de 2020 como coordenadora do Programa de Gestão de Indicadores de Desempenho (GID) da Pro-reitoria de Pós-graduação e Pesquisa da UFRJ. A professora Daniela irá compartilhar conosco sua experiência com o escritório de gestão de indicadores que ela coordena aqui na UFRJ.
  - A terceira apresentação será do Dr. Rogério Mugnaini,
    - Que é estatístico (2001), mestre (2003) e doutor (2006) em Ciência da Informação. Compõe o corpo docente do Departamento de Informação e Cultura da Escola de Comunicações e Artes (ECA) da Universidade de São Paulo (USP), onde coordena o CiMetrias (Grupo de Pesquisa em Métricas da Ciência e Tecnologia). Atua como professor e atual coordenador do Programa de Pós-Graduação em Ciência da Informação da ECA-USP. Realizou estágios de pesquisa nas universidades: Universidad Carlos III de Madrid e Leiden University. Coordenou projetos de pesquisa nacionais e internacionais em temas como: bibliometria, cientometria, avaliação de produção científica nacional, indicadores, fontes de informação, política científica e ciência aberta. Dentre inúmeras outras atividades, integrou comissões avaliativas em âmbito nacional (Avaliações Trienal 2013 e Quadrienais 2017 e 2021 da área de Comunicação e Informação - CAPES). O professor Rogério irá abordar em sua fala os indicadores na avaliação Capes.
  - A quarta e última apresentação desse painel será realizada pela Dra. Samile Vanz,
    - Que é professora associada do Departamento de Ciências da Informação, do Programa de Pós-graduação em Comunicação (PPGCOM UFRGS) e do Programa de Pós-graduação em Ciência da Informação da Universidade Federal do Rio Grande do Sul (PPGCIN UFRGS). Graduada em Biblioteconomia pela Universidade Federal do Rio Grande do Sul (1999), mestre e doutora em Comunicação e Informação pelo PPGCOM UFRGS (2004 e 2009), com estágio sanduíche na Dalian University of Technology (China, 2007-2008). Pós-doutorado pela Universidad Carlos III de Madrid (Espanha, 2016). Desenvolve pesquisas na área de Comunicação Científica, com ênfase na produção de indicadores científicos, bibliometria, colaboração científica, análise de citação, análise de co-citação e rankings universitários. A Profa. Samile discorrerá sobre os rankings internacionais e as universidades brasileiras.
  - Então, dado o calibre dos apresentadores e os temas abordados por ele, certamente essa atividade será muitíssimo interessante e esclarecedora. Lembrando que teremos um espaço para dúvidas e discussões assim que todas as apresentações forem concluídas. Mais uma vez muito obrigado a todos os envolvidos e espero que aproveitem a atividade.

** 1ª palestra: Prof. Jacqueline Leta - Indicadores de desempenho e principais processos de avaliação no Brasil
-
** 2ª palestra: Prof. Daniela Uziel - O escritório de gestão de indicadores da UFRJ
-
** 3ª palestra: Prof. Rogério Mugnaini - Indicadores na avaliação Capes
- Começou com 43 minutos
** 4ª palestra: Prof. Samile Vanz - Os rankings internacionais e as universidades brasileiras

- Muito se fala sobre a relação entre indicadores e a mensuração da qualidade da pesquisa. Existem metricas como o indice h que tentam fazer um mix disso. Entretanto, não importa a métrica/conjunto de métricas.

- Muito se fala sobre como os indicadores podem gerar resultados altamente enviesados. Eu particularmente já ouvi e li muito que o cerne da aplicação de indicadores jaz justamente em selecionar indicadores adequados para contextos/avaliações específicas. Vcs, na condição de estudiosos da cientometria, já se depararam ou deparam com frequência com usos inadequados de indicadores?

- Sobre os rankings: eles são retratos imperfeitos da realidade, concordo. Mas até que ponto vcs acreditam que eles impactam a visão da sociedade para com a instituição, em especial em um momento no qual ainda há uma ataque muito grande às instituições de ensino superior? Os rankings, para alem de uma ferramenta para avaliaçao interna, teria potencial para melhorar a reputação da universidade junto à sociedade, e promover um mecanismo de pressão para evitar o desmonte das instituições de ensino superior?

* Artigo: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic
- Parte que fala das comparações já feitas: Baixar e ler os artigos citados nessa parte.

* Apesentação sobre o Nextcloud
- Mostrar funções extras:
  - Calendario
  - Email
  - Contatos e chat
  - (Claro) Sincronização de arquivos
    - Criando grupos e círculos (diferença entre os dois?)
  - Obs: Segundo a [[https://github.com/nextcloud-snap/nextcloud-snap][página do github]], dá pra exportar e importar os dados do servidor entre versões do snap. Isso acaba com minhas preocupações
- Resolver problemas de segurança:
  - [[https://docs.nextcloud.com/server/24/admin_manual/installation/harden_server.html][Este link]] pode ajudar

* Dia 08/11
